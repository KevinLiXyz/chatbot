"""
èŠå¤©æœºå™¨äººæ ¸å¿ƒæ¨¡å—
Chatbot Core Module with LangChain
"""

from typing import List, Dict
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain_openai import ChatOpenAI

from app.config import (
    OPENAI_API_KEY,
    QW_API_BASE_URL,
    QW_API_KEY,
    QW_Model,
    LLM_TYPE,
    LLM_MODEL,
    OLLAMA_MODEL,
    OLLAMA_BASE_URL,
    TEMPERATURE,
    SYSTEM_PROMPT
)
from app.rag import RAGRetriever


class GovernmentChatbot:
    """æ”¿åŠ¡æ™ºèƒ½å®¢æœæœºå™¨äºº"""
    
    def __init__(self, rag_retriever: RAGRetriever):
        """
        åˆå§‹åŒ–èŠå¤©æœºå™¨äºº
        
        Args:
            rag_retriever: RAGæ£€ç´¢å™¨å®ä¾‹
        """
        self.rag_retriever = rag_retriever
        self.llm = None
        self.memory = None
        self.qa_chain = None
        
        self._initialize_llm()
        self._initialize_memory()
        self._initialize_chain()
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹"""
        
        # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„LLM
        llm_type = LLM_TYPE
        if llm_type == "auto":
            if OPENAI_API_KEY:
                llm_type = "openai"
            elif QW_API_KEY:
                llm_type = "qw"
            else:
                # å°è¯•ä½¿ç”¨Ollama
                llm_type = "ollama"
        
        print(f"ğŸ¤– æ­£åœ¨åˆå§‹åŒ–LLMæ¨¡å‹...")
        print(f"   ç±»å‹: {llm_type}")
        
        try:
            llm_type = "qw"
            if llm_type == "openai":
                # ä½¿ç”¨ OpenAI
                from openai import OpenAI
                
                if not OPENAI_API_KEY:
                    raise ValueError("OpenAI API Keyæœªè®¾ç½®")
                
                print(f"   æ¨¡å‹: {LLM_MODEL}")
                self.llm = ChatOpenAI(
                    model="deepseek-chat",  # æˆ–è€… "deepseek-reasoner" ç”¨äºæ€è€ƒæ¨¡å¼:cite[1]
                    temperature=TEMPERATURE,
                    openai_api_key=OPENAI_API_KEY,
                    openai_api_base="https://api.deepseek.com/v1",  # DeepSeek API ç«¯ç‚¹:cite[1]:cite[5]
                    max_tokens=1024,  # å¯é€‰å‚æ•°
                    timeout=None,
                    max_retries=2
                )
                # self.llm = ChatOpenAI(
                #     model=OLLAMA_MODEL,  # æˆ–è€… "deepseek-reasoner" ç”¨äºæ€è€ƒæ¨¡å¼:cite[1]
                #     temperature=TEMPERATURE,
                #     openai_api_key=OLLAMA_API_KEY,
                #     openai_api_base="https://api.deepseek.com/v1",  # DeepSeek API ç«¯ç‚¹:cite[1]:cite[5]
                #     max_tokens=1024,  # å¯é€‰å‚æ•°
                #     timeout=None,
                #     max_retries=2
                # )
                print("âœ… OpenAIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            elif llm_type == "qw":
                # ä½¿ç”¨ QW å¤§æ¨¡å‹
                if not QW_API_KEY:
                    raise ValueError("QW API Keyæœªè®¾ç½®")
                
                print(f"   æ¨¡å‹: {QW_Model}")
                print(f"   æœåŠ¡: {QW_API_BASE_URL}")
                self.llm = ChatOpenAI(
                    model=QW_Model,
                    openai_api_key=QW_API_KEY,
                    openai_api_base=QW_API_BASE_URL,
                    temperature=TEMPERATURE,
                    timeout=30
                )
                print("âœ… QWå¤§æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")    
            elif llm_type == "ollama":
                # ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹
                try:
                    from langchain_community.llms import Ollama
                    
                    print(f"   æ¨¡å‹: {OLLAMA_MODEL}")
                    print(f"   æœåŠ¡: {OLLAMA_BASE_URL}")
                    self.llm = Ollama(
                        model=OLLAMA_MODEL,
                        base_url=OLLAMA_BASE_URL,
                        temperature=TEMPERATURE,
                        timeout=300
                    )
                    print("âœ… Ollamaæœ¬åœ°æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
                    print("ğŸ’¡ æç¤º: ä½¿ç”¨å…è´¹çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹")
                    
                except Exception as e:
                    print(f"âš ï¸  Ollamaåˆå§‹åŒ–å¤±è´¥: {e}")
                    print("ğŸ’¡ è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨")
                    print("   å®‰è£…: https://ollama.ai")
                    print("   å¯åŠ¨: ollama serve")
                    print(f"   æ‹‰å–æ¨¡å‹: ollama pull {OLLAMA_MODEL}")
                    raise
                    
            elif llm_type == "fake":
                # ä½¿ç”¨å‡çš„LLMç”¨äºæµ‹è¯•
                from langchain_community.llms.fake import FakeListLLM
                
                responses = [
                    "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›ç­”ã€‚å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·é…ç½®OpenAI API Keyæˆ–å®‰è£…Ollamaã€‚",
                    "æˆ‘æ˜¯ä¸€ä¸ªæ¼”ç¤ºç”¨çš„å‡æ¨¡å‹ã€‚è¯·å®‰è£…çœŸå®çš„LLMæ¥è·å¾—æ™ºèƒ½å›ç­”ã€‚",
                    "å½“å‰ä½¿ç”¨æµ‹è¯•æ¨¡å¼ã€‚è¦è·å¾—çœŸå®å›ç­”ï¼Œè¯·é…ç½®LLMã€‚"
                ]
                self.llm = FakeListLLM(responses=responses)
                print("âš ï¸  ä½¿ç”¨æµ‹è¯•æ¨¡å¼(FakeLLM)")
                print("ğŸ’¡ è¦è·å¾—çœŸå®å›ç­”,è¯·é…ç½®OpenAIæˆ–Ollama")
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„LLMç±»å‹: {llm_type}")
                
        except Exception as e:
            print(f"\nâŒ LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            print("\nğŸ’¡ å¯ç”¨é€‰é¡¹:")
            print("   1. é…ç½®OpenAI API Key (æ¨è):")
            print("      åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: OPENAI_API_KEY=sk-xxx")
            print("   2. ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹(å…è´¹):")
            print("      å®‰è£…: https://ollama.ai")
            print("      è¿è¡Œ: ollama pull qwen2.5:7b")
            print("   3. ä½¿ç”¨æµ‹è¯•æ¨¡å¼:")
            print("      åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: LLM_TYPE=fake")
            raise
    
    def _initialize_memory(self):
        """åˆå§‹åŒ–å¯¹è¯è®°å¿†"""
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            output_key="answer",
            return_messages=True
        )
        
        print("ğŸ’­ å¯¹è¯è®°å¿†å·²åˆå§‹åŒ–")
    
    def _initialize_chain(self):
        """åˆå§‹åŒ–å¯¹è¯æ£€ç´¢é“¾"""
        print("ğŸ”— æ­£åœ¨æ„å»ºå¯¹è¯æ£€ç´¢é“¾...")
        
        # è‡ªå®šä¹‰é—®ç­”æç¤ºè¯æ¨¡æ¿
        qa_template = f"""{SYSTEM_PROMPT}

åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜:

{{context}}

ç”¨æˆ·é—®é¢˜: {{question}}

è¯·ç»™å‡ºå‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”:"""
        
        QA_PROMPT = PromptTemplate(
            template=qa_template,
            input_variables=["context", "question"]
        )
        
        # åˆ›å»ºå¯¹è¯æ£€ç´¢é“¾
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.rag_retriever.retriever,
            memory=self.memory,
            return_source_documents=True,
            combine_docs_chain_kwargs={"prompt": QA_PROMPT},
            verbose=False
        )
        
        print("âœ… å¯¹è¯æ£€ç´¢é“¾æ„å»ºå®Œæˆ\n")
    
    def chat(self, user_input: str, show_sources: bool = False) -> Dict:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶è¿”å›å›ç­”
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            show_sources: æ˜¯å¦æ˜¾ç¤ºæ¥æºæ–‡æ¡£
            
        Returns:
            åŒ…å«å›ç­”å’Œæ¥æºçš„å­—å…¸
        """
        if not user_input.strip():
            return {
                "answer": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚",
                "sources": []
            }
        
        # è°ƒç”¨å¯¹è¯é“¾
        result = self.qa_chain.invoke({"question": user_input})
        
        answer = result.get("answer", "æŠ±æ­‰,æˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚")
        source_docs = result.get("source_documents", [])
        
        response = {
            "answer": answer,
            "sources": []
        }
        
        # å¦‚æœéœ€è¦æ˜¾ç¤ºæ¥æº
        if show_sources and source_docs:
            for i, doc in enumerate(source_docs[:3], 1):
                response["sources"].append({
                    "index": i,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                })
        
        return response
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        self.memory.clear()
        print("ğŸ”„ å¯¹è¯å†å²å·²æ¸…ç©º")
    
    def get_chat_history(self) -> List[Dict[str, str]]:
        """
        è·å–å¯¹è¯å†å²
        
        Returns:
            å¯¹è¯å†å²åˆ—è¡¨
        """
        messages = self.memory.chat_memory.messages
        history = []
        
        for msg in messages:
            if hasattr(msg, 'type'):
                role = "ç”¨æˆ·" if msg.type == "human" else "åŠ©æ‰‹"
                history.append({
                    "role": role,
                    "content": msg.content
                })
        
        return history
