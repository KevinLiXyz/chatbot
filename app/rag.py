"""
RAGæ£€ç´¢å¢å¼ºç”Ÿæˆæ¨¡å—
RAG (Retrieval-Augmented Generation) Module
"""

import os
from typing import List, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import chromadb

from app.config import (
    KNOWLEDGE_BASE_PATH,
    EMBEDDING_MODEL,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    VECTOR_STORE_PATH,
    TOP_K_RESULTS
)


class RAGRetriever:
    """RAGæ£€ç´¢å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–RAGæ£€ç´¢å™¨"""
        self.embeddings = None
        self.vector_store = None
        self.retriever = None
        
    def initialize_chromadb(self):
        """åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆ"""
        self.chroma_client = chromadb.PersistentClient('./data/vector_store/chroma_db')
        self.chromadb_collection = self.chroma_client.get_or_create_collection(name="rag_collection")
    def list_files(self, directory: str) -> List[str]:
        """åˆ—å‡ºç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        files = []
        for root, _, filenames in os.walk(directory):
            for filename in filenames:
                files.append(os.path.join(root, filename))
        return files
    
    def load_documents(self) -> List[Document]:
        """åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£"""
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½çŸ¥è¯†åº“: {KNOWLEDGE_BASE_PATH}")
        documents = []
        for file_path in self.list_files(KNOWLEDGE_BASE_PATH):
            print(f"   - {file_path}")
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            loader = TextLoader(file_path, encoding='utf-8')
            documents.extend(loader.load())
            
        print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """å°†æ–‡æ¡£åˆ†å‰²æˆå°å—"""
        print(f"âœ‚ï¸  æ­£åœ¨åˆ†å‰²æ–‡æ¡£ (chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "!", "?", ";", "ï¼›", "!", "?", "ï¼Œ", ",", " ", ""],
        )
        
        chunks = text_splitter.split_documents(documents)
        print(f"âœ… æ–‡æ¡£å·²åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        print(chunks)
        return chunks
    
    def initialize_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL}")
        
        try:
            # å°è¯•åŠ è½½æ¨¡å‹,è®¾ç½®ç¼“å­˜ç›®å½•
            self.embeddings = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True},
                cache_folder="./.cache"  # ä½¿ç”¨æœ¬åœ°ç¼“å­˜ç›®å½•
            )
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹åŠ è½½é‡åˆ°é—®é¢˜: {str(e)[:100]}")
            print("ğŸ’¡ å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
            
            # å¤‡ç”¨æ–¹æ¡ˆ: ç›´æ¥ä½¿ç”¨SentenceTransformer
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(EMBEDDING_MODEL)
            
            # åŒ…è£…ä¸ºLangChainå…¼å®¹çš„åµŒå…¥ç±»
            class CustomEmbeddings:
                def __init__(self, model):
                    self.model = model
                
                def embed_documents(self, texts):
                    return self.model.encode(texts, convert_to_numpy=True).tolist()
                
                def embed_query(self, text):
                    return self.model.encode([text], convert_to_numpy=True)[0].tolist()
            
            self.embeddings = CustomEmbeddings(model)
            print("âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹æˆåŠŸ")
    
    def build_vector_store(self, chunks: List[Document]):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        print("ğŸ—ï¸  æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“...")
        
        self.vector_store = FAISS.from_documents(
            documents=chunks,
            embedding=self.embeddings
        )
        
        print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ")
    
    def save_vector_store(self):
        """ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç£ç›˜"""
        if self.vector_store is None:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)
        self.vector_store.save_local(VECTOR_STORE_PATH)
        print(f"ğŸ’¾ å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {VECTOR_STORE_PATH}")
    def build_vector_store_chroma(self, chunks: List[Document]):
        """ä¿å­˜å‘é‡æ•°æ®åº“åˆ°ç£ç›˜"""
        if self.vector_store is None:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        for i, (chunk, embedding) in enumerate(zip(chunks, self.embeddings)):
            chromadb_collection.add(
                documents=[chunk],
                embeddings=[embedding],
                ids=[str(i)]
            )
        print(f"ğŸ’¾ å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: data/vector_store/chroma_db")
    def load_vector_store(self):
        """ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“"""
        if not os.path.exists(VECTOR_STORE_PATH):
            raise FileNotFoundError(f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {VECTOR_STORE_PATH}")
        
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {VECTOR_STORE_PATH}")
        self.initialize_embeddings()
        
        self.vector_store = FAISS.load_local(
            VECTOR_STORE_PATH,
            self.embeddings,
            # allow_dangerous_deserialization=True
        )
        
        print("âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ")
        def load_vector_store_from_chromas(self):
            """ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“"""
            if not os.path.exists(VECTOR_STORE_PATH):
                raise FileNotFoundError(f"å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {VECTOR_STORE_PATH}")
            
            print(f"ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {VECTOR_STORE_PATH}")
            self.initialize_embeddings()
            
            self.vector_store = FAISS.load_local(
                VECTOR_STORE_PATH,
                self.embeddings,
                # allow_dangerous_deserialization=True
            )
            
            print("âœ… å‘é‡æ•°æ®åº“åŠ è½½å®Œæˆ")
        
    def setup_retriever(self):
        """è®¾ç½®æ£€ç´¢å™¨"""
        if self.vector_store is None:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        self.retriever = self.vector_store.as_retriever(
            search_kwargs={"k": TOP_K_RESULTS}
        )
        
        print(f"ğŸ” æ£€ç´¢å™¨å·²é…ç½® (top_k={TOP_K_RESULTS})")
    
    def initialize(self, force_rebuild: bool = False):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºå‘é‡æ•°æ®åº“
        """
        print("ğŸš€ åˆå§‹åŒ–RAGæ£€ç´¢ç³»ç»Ÿ...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºå‘é‡æ•°æ®åº“
        if force_rebuild or not os.path.exists(VECTOR_STORE_PATH):
            print("ğŸ“š å¼€å§‹æ„å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
            
            # åŠ è½½å’Œå¤„ç†æ–‡æ¡£
            documents = self.load_documents()
            chunks = self.split_documents(documents)
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            self.initialize_embeddings()
            
            # æ„å»ºå¹¶ä¿å­˜å‘é‡æ•°æ®åº“
            self.build_vector_store(chunks)
            self.save_vector_store()
        else:
            # åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“
            self.load_vector_store()
        
        # è®¾ç½®æ£€ç´¢å™¨
        self.setup_retriever()
        
        print("âœ… RAGæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!\n")
    
    def retrieve(self, query: str) -> List[Document]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if self.retriever is None:
            raise ValueError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
        
        results = self.retriever.get_relevant_documents(query)
        return results
    
    def retrieve_with_scores(self, query: str) -> List[Tuple[Document, float]]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£åŠå…¶ç›¸ä¼¼åº¦åˆ†æ•°
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            (æ–‡æ¡£, åˆ†æ•°)å…ƒç»„åˆ—è¡¨
        """
        if self.vector_store is None:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        results = self.vector_store.similarity_search_with_score(
            query,
            k=TOP_K_RESULTS
        )
        
        return results
